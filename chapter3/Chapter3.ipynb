{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "# X is a pd.DataFrame, but we need a numpy array\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "print(type(y))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample = np.random.randint(0, X.shape[0] - 1)\n",
    "some_digit = X[sample]\n",
    "some_digit_reshape = some_digit.reshape(\n",
    "    tuple(\n",
    "        [np.sqrt(X.shape[1]).astype(\"int\")]*2\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(some_digit_reshape, cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = 0.8\n",
    "limit = round(X.shape[0]*train_test_ratio)\n",
    "# the dataset is already shuffled\n",
    "X_train, X_test, y_train, y_test = X[:limit], X[limit:], y[:limit], y[limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: some data exploration to check categories are correctly shuffled\n",
    "import seaborn as sns\n",
    "\n",
    "sns.displot(y_train)\n",
    "sns.displot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform target vector\n",
    "y_train_5 = y_train == 5\n",
    "y_test_5 = y_test == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocastic gradient descent classifier (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation from scratch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    print(f\"train_index: {train_index}\")\n",
    "    print(f\"test_index: {test_index}\")\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train_5[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_5[test_index]\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(f\"fold accuracy: {n_correct / len(y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn built-in function for cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy classifies\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y): \n",
    "        pass\n",
    "    def predict(self, X): \n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never5classifier = Never5Classifier()\n",
    "\n",
    "cross_val_score(never5classifier, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "# cross_val_predict is similat to cross_val_score, but it returns \n",
    "# predicted values in each test fold\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create first confusion matrix by hand\n",
    "\n",
    "positives_index = np.flatnonzero(y_train_pred)\n",
    "negatives_index = np.flatnonzero(~y_train_pred)\n",
    "TP = sum(y_train_5[positives_index])\n",
    "FP = len(positives_index) - TP\n",
    "TN = sum(~y_train_5[negatives_index])\n",
    "FN = len(negatives_index) - TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = np.array([[TN, FP], [FN, TP]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sklearn \n",
    "from sklearn.metrics import confusion_matrix\n",
    "bi_conf_matrix = confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_matrix)\n",
    "print(bi_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us compute precision and recall by hand and with sklearn built-in functions\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = TP / (TP + FP)\n",
    "bi_precision = precision_score(y_train_5, y_train_pred)\n",
    "recall = TP / (TP + FN)\n",
    "bi_recall = recall_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hand made precision: {precision:0.2f}\")\n",
    "print(f\"Built in precision: {bi_precision:0.2f}\")\n",
    "print(f\"Hand made recall: {recall:0.2f}\")\n",
    "print(f\"Built in recall: {bi_recall:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = 2 / (1/precision + 1/recall)\n",
    "bi_f1 = f1_score(y_train_5, y_train_pred)\n",
    "print(f\"Hand made F1 score: {f1:0.2f}\")\n",
    "print(f\"Built in F1 score: {bi_f1:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The precision/recall trade off\n",
    "\n",
    "In sklearn, classifiers have a method called `decision_function` that gives us the score a certain instance has. We classify our inputs depending of this score being higher than a certain therhold. For example, the `SGDClassifier` uses a threshold equal to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scores = sgd_clf.decision_function(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_train_pred_2 = (y_train_scores > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = sgd_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(y_train_pred == y_train_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds, point_precision=None):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    if (point_precision):\n",
    "        point_recall = recalls[np.argmax(precisions >= point_precision)]\n",
    "        point_threshold = thresholds[np.argmax(precisions >= point_precision)]\n",
    "        plt.plot([point_threshold, point_threshold], [0., point_precision], \"r:\")                 \n",
    "        plt.plot([-50000, point_threshold], [point_precision, point_precision], \"r:\")                                \n",
    "        plt.plot([-50000, point_threshold], [point_recall, point_recall], \"r:\")\n",
    "        plt.plot([point_threshold], [point_precision], \"ro\")                                             \n",
    "        plt.plot([point_threshold], [point_recall], \"ro\")\n",
    "    plt.legend(loc=\"center right\", fontsize=16) \n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        \n",
    "    plt.grid(True)                              \n",
    "    plt.axis([-50000, 50000, 0, 1]) \n",
    "    plt.title(\"Precision/Recall curve\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))    \n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds, point_precision=0.8)                            \n",
    "save_fig(\"precision_recall_curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls, point_precision=None):\n",
    "    plt.plot(precisions, recalls, \"b--\", linewidth=2)\n",
    "    if (point_precision):\n",
    "        point_index = np.argmin(np.abs(precisions - 0.8))\n",
    "        point_precision = precisions[point_index]\n",
    "        point_recall = recalls[point_index]\n",
    "        plt.plot([point_precision, point_precision], [0, point_recall], \"r:\", linewidth=2)\n",
    "        plt.plot([0, point_precision], [point_recall, point_recall], \"r:\", linewidth=2)\n",
    "        plt.plot([point_precision], [point_recall], \"ro\")\n",
    "    plt.xlabel(\"Precision\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Precision vs Recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plot_precision_vs_recall(precisions, recalls, 0.8)\n",
    "save_fig(\"precision_recall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve\n",
    "Another common tool when using binary classifiers is the so called **ROC curve** *(receiver operating characteristic)* which plots TPR vs FPR. \n",
    "- TPR (true positive rate): also called sensitivity or recall, is the ratio of real positives that have been correctly classified\n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "- FPR (false positive rate): is the ratio of real negatives that has been incorrectly classified as positives\n",
    "$$\n",
    "FPR = \\frac{FP}{FP + TN} = 1 - \\frac{TN}{FP + TN} = 1 - \\textit{specificity}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thersholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.grid()\n",
    "    plt.title(\"ROC curve\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC score\n",
    "One way to evaluate classifiers is to look a the AUC (area under the curve). The higher the area, the best a classifier will performe. Sklearn provides a function to compute the ROC AUC, `roc_auc_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "sgd_auc = roc_auc_score(y_train_5, y_scores)\n",
    "print(sgd_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve vs PR Curve\n",
    "The key point when evaluating a classifier is to decide which is more important, false positives or false negatives. For example,\n",
    "- Predict cancer on patients: minimize false negatives.\n",
    "- Detect safe videos for children: minimize false positives.\n",
    "> You should use PR Curve whenever the positive class is rare or when you care more about the false positives than the false negatives. Otherwise, use the ROC curve and de AUC score. \n",
    "\n",
    "In the detect 5 digits problem, looking at the AUC we could conclude there is no room for improvement (because there are few 5s (positives)), but the PR curve makes it clear we can do better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train another classifier (in this case a random forest classifier) and compare its ROC curve with the Stocastic Gradient Descend Classifier. \n",
    "\n",
    "`RandomForestClassifier` does not have a `decision_function` method like `SGDClassifier`. Instead, it has a `predict_proba` method. Sklearn classifiers generally have one or the other, or both. The `predict_proba` method returns an array containing a row per instance and a column per class, each containing the probability that the fiven instance belongs to the given class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1] # probability of positive class\n",
    "fpr_forest, tpr_forest, thersholds_forest = roc_curve(y_train_5, y_scores_forest)\n",
    "precision_forest, recall_forest, thersholds_forest = precision_recall_curve(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(fpr_forest, tpr_forest, linewidth=2, label=\"Random Forest\")\n",
    "plot_roc_curve(fpr, tpr, label=\"SGD\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_auc = roc_auc_score(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision/recall of Random Forest Classifier\n",
    "y_pred_forest = y_probas_forest[:, 1] > 0.5\n",
    "forest_precision = precision_score(y_train_5, y_pred_forest)\n",
    "forest_recall = recall_score(y_train_5, y_pred_forest)\n",
    "sgd_precision = precision\n",
    "sgd_recall = recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics_data = {\n",
    "    \"Forest\": [forest_auc, forest_precision, forest_recall], \n",
    "    \"SGD\": [sgd_auc, sgd_precision, sgd_recall]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(\n",
    "    metrics_data, index=[\"auc\", \"precision\", \"recall\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format pandas table\n",
    "\n",
    "def make_pretty(styler):\n",
    "    styler.set_caption(\"Metric comparison between classfiers\")\n",
    "    styler.background_gradient(axis=None, vmin=0, vmax=1, cmap=\"YlGnBu\")\n",
    "    return styler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.style.pipe(make_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass clasification\n",
    "\n",
    "Some algorithms (such as Stocastic Gradient Descend, Random Forest or Naive Bayes classifiers) are capable of handling multiple classes natively. Others, like Logistic Regression or Support Vector Machine classifiers are stricly binary, but we can turn them into multiclass classifiers with the following strategies: \n",
    "1. *one-versus-all* or *one-versus-the-rest* (OvR): train a binary classifier for each class and predict the class whose score is maximum. \n",
    "2. *one-versus-one* (OvO): train a binary classifier for each pair of classes, that is, $N \\times (N - 1) / 2$ classifiers and predict the class which wins the most duels. You need to train more models, but the training sets get smaller since you only need to retain two classes from the whole dataset. This is convenient for classifier that scale poorly with the size of the triainig set, such as Support Vector Machines. Otherwise, OvR is preferred.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn detects when a binary classifier is used for multiclass detection and automatically runs OvR or OvO depeding on the the lgorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, sklearn trained 45 classifiers with a OvO strategy. Let us have a look at the `decision_function` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = svm_clf.decision_function([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(some_digit_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the `classes_` atribute matches the digit they represent, but this is not going to happend in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ovr stretegy\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "ovr_clf = OneVsRestClassifier(SVC())\n",
    "ovr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# SGD can predict multiple classes\n",
    "sgd_clf = SGDClassifier()\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skelearn.preprocessing import StandardScaler \n",
    "standard_scaler = StandardScaler()\n",
    "X_train_scaled = standard_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a multiclass classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel classification\n",
    "Sometimes we want to predict not only one class, but many of them. Let us take a look to a quick example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2)\n",
    "knn_clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_multilabel = np.c_[y_train_large, y_train_odd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56000, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_multilabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf.fit(X_train, y_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('my_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "55399731cebb31993cda92c257ac89f2c586188a1be3b67becd9ee94b331b4e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
