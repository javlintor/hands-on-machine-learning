{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning and Random Forests\n",
    "\n",
    "## Voting classifiers\n",
    "When your task is to classify some instances, one strategy is to make use of the **law of large numbers** and combine some classifiers to predict the most common class between them. This is an example of an esamble method, and works great when models predictors are independent\n",
    "\n",
    "\n",
    "## Bagging and pasting \n",
    "Another common strategy is to use the same model for different predictors trained on random samples of the data. \n",
    "- **Bagging**: With replacement\n",
    "- **Pasting**: Without replacement\n",
    "When using these techniques, you can performe a **Out-of-Bag Evaluation**, using data that wasnt used for training not having to prepare a validation test. You can evaluate the ensemble by averaging over the oob evaluations.\n",
    "\n",
    "## Random Forests \n",
    "A Random Forest is a ensemble of decision trees generally trained via bagging. sklearn incorporates special methods for Random Trees as well as a optimized implementation which works faster than Bagging a emsable of Decision Trees. \n",
    "\n",
    "## Boosting \n",
    "The idea of Boosting is to train predictor sequentially, focusing on the weeknesses of previous predictors. There are two main algorithms: \n",
    "- **AdaBoost**: Here we pay attention to the training instances that a predecessor underfitted. For example, when working with decission tree we can increase the weight of missclassified training instances. \n",
    "- **Gradient Boosting**: The idea of gradient boosting is to train a predictor on the residual errors of the preceding one. There is an optimized implementation of this algorithm at a popular python library called `XGBoost`. \n",
    "\n",
    "##Â Stacking\n",
    "It is a method which consists of training a model to aggregate predictions made by a set of predictors. The predictor in charge of aggregation is called a *blender*, and it is common to split the training set into two smaller sets, the first one is used to train the first layer predictors and the second one is used to train the blender after being transformed as predictions by the first layer. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
